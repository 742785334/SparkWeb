用户行为日志(用户行为轨迹、流量日志)：用户每次访问网站时所有的行为数据（访问、浏览、搜索、点击...）

日志产生渠道：nginx，ajax
日志数据内容：
1）访问的系统属性： 操作系统、浏览器等等
2）访问特征：点击的url、从哪个url跳转过来的(referer)、页面上的停留时间等
3）访问信息：session_id、访问ip(访问城市)等

2013-05-19 13:00:00     http://www.taobao.com/17/?tracker_u=1624169&type=1      B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1        http://hao.360.cn/      1.196.34.243   


数据处理流程
1）数据采集
	Flume： web日志写入到HDFS（从一个地方迁移到另一个地方 ）

2）数据清洗
	脏数据
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架  
	清洗完之后的数据可以存放在HDFS(Hive/Spark SQL)

3）数据处理
	按照我们的需要进行相应业务的统计和分析
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架

4）处理结果入库
	结果可以存放到RDBMS、NoSQL

5）数据的可视化
	通过图形化展示的方式展现出来：饼图、柱状图、地图、折线图
	ECharts、HUE、Zeppelin
	
Oozie,Azkaban 调度执行框架

--------------------------------------------------------------
访问时间，访问URL，访问过程耗费流量，访问IP地址

【第一步解析】
access.map(line =>{
val splits=line.split(" ")
val ip=splits(0)
//日期转换
val time=splits(3)+" "+splits(4)


val url = splits(11).replaceAll("\"","")
val traffic =splits(9)

DateUtils.parse(time)+"\t"+url+"\t"+traffic
}).take(10).foreach(println).saveAsTextFile("...")

object DateUtils｛
  //SimpleDateFormat是线程不安全的
  val YYYYMMDDHHMM_TIME_FORMAT=FastDateFormat("dd/MMM/yyyy:HH:mm:ss Z",Locale.ENGLISH)
  val TARGET_FORMAT=FastDateFormat("yyyy-MM-dd HH:mm:ss")
  
  def parse(time:String)={
  TARGET_FORMAT.format(new Date(getTime(time)))
  }
  
  def getTime(time:String)={
  try{
      YYYYMMDDHHMM_TIME_FORMAT.parse(time.substring(time.indexOf("[")+1,time.lastIndexOf("]"))).getTime
  } catch{
	case e:Exception =>{
		0l
	}
  }
}
}

//more part-000*

一般的日志处理方式，我们是需要进行分区的，
按照日志中的访问时间进行相应的分区，比如：d,h,m5(每5分钟一个分区)

--------------------------------------------------------------
【输入】：访问时间、访问URL、耗费的流量、访问IP地址信息
【输出】：URL、cmsType(video/article)、cmsId(编号)、流量、ip、城市信息、访问时间、天

object SparkStatCleanJob{
	main{
		sparkSession
		val accessRDD=spark.sparkContext.textFile()
		
		val accessDF = spark.createDataFrame(accessRDD.map(
		x => AccessConvertUtil.parseLog(x),AccessConvertUtil.StructType
		))
	//accessDF.printSchema()
	//accessDF.show(false)
	accessDF.coalesce(1).write.format("parquet").patitionBy("day").mode(SaveMode.Overwirte).save("路径")
	}
}

object AccessConvertUtil{
	
	//定义输出字段
	val struct=StructType(
	Array(StructField("url",StringType),
	StructField("cmsType",StringType),
	StructField("cmsId",LongType),
	StructField("traffic",LongType),
	StructField("ip",StringType),
	StructField("city",StringType),
	StructField("time",StringType),
	StructField("day",StringType)
	)
)

	def parseLog(log:String)={
		val splits = log.split("\t")
		val url=splits(1)
		val traffic =splits(2).toLong
		val ip =splits(3)
		
		val domain="http://www.imooc.com"
		val cms=url.substring(url.indexOf(domain)+domain.length)
		val cmsTypeId=cms.split("/")
		
		val cmsType= if (cmsTypeId.length>1) cmsTypeId(0) else ""
		val cmsId=if (cmsTypeId.length>1) cmsTypeId(1).toLong else ""
		val day=time.substring(0,10).replaceAll("","|")
		
		val city=""//IpUtils
	
		Row(url,cmsType,cmsId,traffic,ip,city,time,day)
	}

}
---------------------------------------------------------------
使用github上已有的开源项目
1）git clone https://github.com/wzhe06/ipdatabase.git
2）编译下载的项目：mvn clean package -DskipTests
3）安装jar包到自己的maven仓库
mvn install:install-file -Dfile=E:\\ipdatabase\target\ipdatabase-1.0-SNAPSHOT.jar -DgroupId=com.ggstar -DartifactId=ipdatabase -Dversion=1.0 -Dpackaging=jar
4）加入依赖
<dependency>
            <groupId>com.ggstar</groupId>
            <artifactId>ipdatabase</artifactId>
            <version>1.0</version>
        </dependency>
        <dependency>
            <groupId>com.apache.poi</groupId>
            <artifactId>poi-ooxml</artifactId>
            <version>3.14</version>
        </dependency>
        <dependency>
            <groupId>com.apache.poi</groupId>
            <artifactId>poi</artifactId>
            <version>3.14</version>
        </dependency>

object IpUtils{
	def getCity(ip:String)={
		IpHelper.findRegionByIp("xxx")
	}
}

java.io.FileNotFoundException: 
file:/Users/rocky/maven_repos/com/ggstar/ipdatabase/1.0/ipdatabase-1.0.jar!/ipRegion.xlsx (No such file or directory)
把源码中的resources下文件拷贝过来

调优点：
1) 控制文件输出的大小： coalesce方法
2) 分区字段的数据类型调整：spark.sql.sources.partitionColumnTypeInference.enabled
3) 批量插入数据库数据，提交使用batch操作

.config("spark.sql.sources.partitionColumnTypeInference.enabled","false")
videoAccessTopNStat(spark,accessDF)
//统计video的topN
def videoAccessTopStat(spark:SparkSession,accessDF:DataFrame){
	import spark.implicits._
	val accessTop = accessDF.filter($"day"==="20170511" && $"cmsType"==="video").groupBy("day","cmsId").agg(count("cmsId").as("times")).orderBy($"times".desc)
	accessTop.show(false)
	
	accessDF.createOrReplaceTempView("access_logs")
	val videoAccessTopDF = spark.sql("select day,cmsId,count(1) as times from access_logs where day='20170511' and cmsType='viedo' group by day,cmsId order by times desc")
	videoAccessTopDF.show(false)
	videoAccessTopNDF.foreachPartition(partitionOfRecords =>{
		val list = new ListBuffer[DayVideoAccessStat]
		
		partitionOfRecords.foreach(info =>{
			val day = info.getAs[String]("day")
			val cmsId = info.getAs[Long]("cmsId")
			val times = info.getAs[Long]("times")
			
			list.append(DayVideoAccessStat(day,cmsId,times))
		})
		StatDAO.insertDayVideoAccessTopN(list)
		
	})
}
-------------------------------------------------------------
操作Mysql
object MySQLUtils{

	def getConnection()={
		DriverManager.getConnection("jdbc:mysql://localhost:3306/imooc_project?user=root&password=123")
	}
	
	def release(connection:Connection,pstmt:PreparedStatement){
		pstmt.close()
		connection.close()
	}
}

create table day_video_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
times bigint(10) not null,
primary key (day, cms_id)
);


create table day_video_city_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
city varchar(20) not null,
times bigint(10) not null,
times_rank int not null,
primary key (day, cms_id, city)
);

create table day_video_traffics_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
traffics bigint(20) not null,
primary key (day, cms_id)
);

case class DayVideoAccessStat(day:String,cmsId:Long,times:Long)

//统计各个维度的dao
object StatDAO{

	def insertDayVideoAccessTopN(list:ListBuffer[DayVideoAccessStat])={
		var connection:Connection=null
		var pstmt:PreparedStatement=null
		try{
			connection=MySQLUtils.getConnection()
			connection.setAtutoCommit(false)//设置手动提交
			val sql ="insert into day_video_access_topn_stat(day,cms_id,times) values (?,?,?)"
			pstmt = connection.prepareStatement(sql)
			
			for(ele <- list){
				pstmt.setString(1,ele.day)
				pstmt.setLong(2,ele.cmsId)
				pstmt.setLong(3,ele.times)
				pstmt.addBathc()
			}
			pstmt.executeBath()//执行批量处理
			connection.commit()
		}catch{
			case e:Exception => e.printStackTrace()
		}finally{
			MySQLUtils.release(connection,pstmt)
		}
	
	}
}

case class DayVideoTrafficsStat(day:String,cmsId:Long.traffics:Long)


//删除指定日期的数据
def deleteData(day:String)={
	val tables=Array("","","")
	var connection:Connection=null
	var pstmt:PreparedStatement=null
	
	for(table<-tables){
		val deleteSQL=s"delete from $table where day=?"
		connection.prepareStatement(deleteSQL)
		pstmt.setString(1,day)
		pstmt.executeUpdate()
	}
}

---------------------------------------------------------------------------------------------------
数据可视化：一副图片最伟大的价值莫过于它能够使得我们实际看到的比我们期望看到的内容更加丰富

创建web项目（webapp）js文件拷贝
｛

	代码......
｝

<dependency>
	<groupId>javax.servlet</groupId>
	<artifactId>servlet-api</artifactId>
	<version>2.5</version>
</dependency>

<dependency>
	<groupId>javax.servlet</groupId>
	<artifactId>jsp-api</artifactId>
	<version>2.0</version>
</dependency>

<dependency>
	<groupId>mysql</groupId>
	<artifactId>mysql-connector-java</artifactId>
	<version>5.1.38</version>
</dependency>

<dependency>
	<groupId>net.sf.json-lib</groupId>
	<artifactId>json-lib</artifactId>
	<version>2.4</version>
	<classifier>jdk15</classifier>
</dependency>

java文件夹，点击sources

//最受欢迎的TOPN课程
public class VideoAccessTopNServlet extends HttpServlet{

	doGet{
	
	}
	
	doPost{
	
	
	}

}

MySQLUtils 文件 获取连接，释放资源

VideoAccessTopN  实体类 
｛
	private String name;
	private long value;
｝

VideoAccessTopNDAO{
	
	public List<xx> query(String day){......}
	
}
//初始化方法
init(){}

//doGet
String day = req.getParameter("day")

List<VideoAccessTopN> results = dao.query(day);
JSONArray json= JSONArray.fromObject(results);

resp.setContentType("text/html;charset=utf-8");
PrintWriter writer = resp.getWriter();
writer.println(json)
writer.flush();
writer.close();

web.xml中注册servlet
<servlet>
	<servlet-name>stat</servlet-name>
	<servlet-class>com.imooc.web.VideoAccessTopNServlet</servlet-class>
</servlet>

<servlet-mapping>
	<servlet-name>stat</servlet-name>
	<url-pattern>/stat</url-pattern>
</servlet-mapping>

jquey.js 拷过来

data:(function(){
	var courses = [];
	$.ajax({
		type:"GET",
		url:"stat?day=20170511",
		dataType:"json",
		async:false,
		success:function(result){
			for(var i=0;i<result.length;i++){
				courses.push({"value":result[i].value, "name":result[i].name})
			}
		}
	})
	return courses
})(),

html文件中把官方文档中的内容放进去

------------------------------------------------------
(1)下载zeeplin 
(2)启动zeeplin
(3)8080端口访问
(4)conf/shiro.ini 可以配置权限
(5)配置mysql访问
	com.mysql.jdbc.DriverManager
	root
	jdbc:mysql://localhost:3306/imooc_project
	123
	mysql驱动

(6)new notebook
%jdbc
sql语句



常见的可视化框架
1）echarts
2）highcharts
3）D3.js
4）HUE 
5）Zeppelin

在Spark中，支持4种运行模式：
1）Local：开发时使用
2）Standalone： （不推荐）是Spark自带的，如果一个集群是Standalone的话，那么就需要在多台机器上同时部署Spark环境
3）YARN：建议大家在生产上使用该模式，统一使用YARN进行整个集群作业(MR、Spark)的资源调度
4）Mesos

不管使用什么模式，Spark应用程序的代码是一模一样的，只需要在提交的时候通过--master参数来指定我们的运行模式即可

Client
	Driver运行在Client端(提交Spark作业的机器)
	Client会和请求到的Container进行通信来完成作业的调度和执行，Client是不能退出的
	日志信息会在控制台输出：便于我们测试

Cluster
	Driver运行在ApplicationMaster中
	Client只要提交完作业之后就可以关掉，因为作业已经在YARN上运行了
	日志是在终端看不到的，因为日志是在Driver上，只能通过yarn logs -applicationIdapplication_id


./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.1.0.jar \
4


此处的yarn就是我们的yarn client模式
如果是yarn cluster模式的话，yarn-cluster


Exception in thread "main" java.lang.Exception: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.

如果想运行在YARN之上，那么就必须要设置HADOOP_CONF_DIR或者是YARN_CONF_DIR
1) export HADOOP_CONF_DIR=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop
2) $SPARK_HOME/conf/spark-env.sh
 

./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn-cluster \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.1.0.jar \
4


yarn logs -applicationId application_1495632775836_0002

----------------------------------------------------------
修改代码 SparkStatCleanJobYARN

	if(args.length!=2){
		println("输入错误")
		System.exit(1)
	}
	
	val Array(inputPath,outputPath) = args
	
	textFile(inputPath)
	
	save(outputPath)


打包时要注意，pom.xml中需要添加如下plugin
<plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
        <archive>
            <manifest>
                <mainClass></mainClass>
            </manifest>
        </archive>
        <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
    </configuration>
</plugin>

mvn assembly:assembly



./bin/spark-submit \
--class com.imooc.log.SparkStatCleanJobYARN \
--name SparkStatCleanJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
--files /home/hadoop/lib/ipDatabase.csv,/home/hadoop/lib/ipRegion.xlsx \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/input/* hdfs://hadoop001:8020/imooc/clean

注意：--files在spark中的使用
 
启动spark-shell
spark.read.format("parquet").load("/imooc/clean/day=20170511/part-00000-71d465d1-7338-4016-8d1a-729504a9f95e.snappy.parquet").show(false)


./bin/spark-submit \
--class com.imooc.log.TopNStatJobYARN \
--name TopNStatJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/clean 20170511 

存储格式的选择：http://www.infoq.com/cn/articles/bigdata-store-choose/
压缩格式的选择：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-compression-analysis/

指定压缩格式：.config("spark.sql.parquet.compression.codec","gzip")

sparksql调优：
(1)增加并行度 spark.sql.shuffle.partitions
(2)分区字段类型推测：spark.sql.sources.partitionColumnTypeInference.enabled

调整并行度
./bin/spark-submit \
--class com.imooc.log.TopNStatJobYARN \
--name TopNStatJobYARN \
--master yarn \
--executor-memory 1G \
--num-executors 1 \
--conf spark.sql.shuffle.partitions=100 \
/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \
hdfs://hadoop001:8020/imooc/clean 20170511 




































